{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = open(\"names.txt\", \"r\").read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "str = \".abcdefghijklmnopqrstuvwxyz\"\n",
    "stoi = {}\n",
    "itos = {}\n",
    "arr = list(str)\n",
    "for idx, char in enumerate(arr):\n",
    "    stoi[char] = idx\n",
    "    itos[idx] = char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([196113, 3])\n"
     ]
    }
   ],
   "source": [
    "# Build the Dataset\n",
    "\n",
    "xs = [] \n",
    "ys = []\n",
    "context_window = 3 # Context length: Number of characters do we take to predict the next one.\n",
    "for name in names:\n",
    "  \n",
    "  context = [0] * context_window # 0 is the index of the delimiter. The context window is considered to consist of only delimiters initially.\n",
    "  \n",
    "  name = name + \".\"\n",
    "\n",
    "  for idx in range(len(name) - 1):\n",
    "  \n",
    "    ix = stoi[name[idx]]\n",
    "    context = context[1:] + [ix] # crop and append\n",
    "    xs.append(context)\n",
    "    ys.append(stoi[name[idx + 1]])\n",
    "    # print(''.join(itos[i] for i in context), '--->', name[idx + 1])\n",
    "  \n",
    "xs = torch.tensor(xs)\n",
    "print(xs.shape)\n",
    "ys = torch.tensor(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "vector_dimensionality = 10\n",
    "char_vectors = torch.randn((27, vector_dimensionality), generator=gen) # This represents the collection of Alphabets in their vector format. 27 Characters are represented as 2D vectors.\n",
    "W1 = torch.randn((vector_dimensionality * context_window, 200), generator=gen) # Shape is (Ctx window * Alphabet Vector Dims, # Neurons). Here, # Neurons = 200 \n",
    "B1 = torch.randn(200, generator=gen)\n",
    "W2 = torch.randn((200, 27), generator=gen)\n",
    "B2 = torch.randn(27, generator=gen)\n",
    "parameters = [char_vectors, W1, B1, W2, B2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in parameters:\n",
    "  param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Training loop for training over the entire dataset. Inherently slow\n",
    "\n",
    "# for _ in range(100):\n",
    "    \n",
    "#     # Forward Pass\n",
    "#     emb = char_vectors[xs] # This returns the collection of vectors for each input context. Each Context is of length = Context length. For each context character, there is a 2D vector. So for each context, the output is Context length x Vector dimensionality. And for all input contexts, the shape is [# of contexts x context length x Vector dimensionality = 2].\n",
    "\n",
    "#     tan_h = torch.tanh(emb.view(-1, context_window * vector_dimensionality) @ W1 + B1)\n",
    "#     output = tan_h @ W2 + B2\n",
    "#     loss = torch.nn.functional.cross_entropy(output, ys)\n",
    "#     print(loss.item())\n",
    "\n",
    "#     # Backward Pass\n",
    "#     for param in parameters:\n",
    "#         param.grad = None\n",
    "\n",
    "#     loss.backward()\n",
    "\n",
    "#     # Update\n",
    "#     for param in parameters:\n",
    "#         param.data -= 0.1 * param.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop for training over batches of the dataset. Faster but maybe less accurate\n",
    "\n",
    "for _ in range(1000):\n",
    "    \n",
    "    indices = torch.randint(0, xs.shape[0], (1024, )) # Returns a list of indices to select a few examples from the entire input dataset\n",
    "\n",
    "    # Forward Pass\n",
    "\n",
    "    # xs[indices] returns a tensor of inputs with only those indices\n",
    "\n",
    "    emb = char_vectors[xs[indices]] # This returns the collection of vectors for specific contexts. Each Context is of length = Context length. For each context character, there is a 2D vector. So for each context, the output is Context length x Vector dimensionality. And for all input contexts, the shape is [# of contexts x context length x Vector dimensionality = 2].\n",
    "\n",
    "    tan_h = torch.tanh(emb.view(-1, context_window * vector_dimensionality) @ W1 + B1) # @ is the matrix multiplication operator\n",
    "    output = tan_h @ W2 + B2\n",
    "    loss = torch.nn.functional.cross_entropy(output, ys[indices])\n",
    "    # print(loss.item())\n",
    "\n",
    "    # Backward Pass\n",
    "    for param in parameters:\n",
    "        param.grad = None\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    # Update\n",
    "    for param in parameters:\n",
    "        param.data -= 0.1 * param.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.0342, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "erbm\n",
      "imiazehrtlend\n",
      "ei\n",
      "ameethaniend\n",
      "ari\n",
      "udereo\n",
      "elii\n",
      "lly\n",
      "odna\n",
      "ieilocoenarteat\n",
      "ivsytes\n",
      "iosadher\n",
      "ahim\n",
      "ele\n",
      "in\n",
      "oselyn\n",
      "anteona\n",
      "ubewaeder\n",
      "ameul\n",
      "eka\n",
      "lay\n",
      "hahanien\n",
      "hyis\n",
      "arlansyn\n",
      "unzomueu\n",
      "ri\n",
      "ca\n",
      "aki\n",
      "eahar\n",
      "amilin\n",
      "eyko\n",
      "or\n",
      "hdneila\n",
      "amaezia\n",
      "amieonlin\n",
      "airi\n",
      "eva\n",
      "htralaiirta\n",
      "atti\n",
      "ee\n",
      "oelisatia\n",
      "at\n",
      "avis\n",
      "reu\n",
      "oelirreckenn\n",
      "yxkir\n",
      "uhmii\n",
      "ie\n",
      "rakyyilo\n",
      "udt\n",
      "iwtd\n",
      "ili\n",
      "arle\n",
      "adalena\n",
      "eni\n",
      "are\n",
      "ina\n",
      "icearilon\n",
      "et\n",
      "aynerekian\n",
      "ire\n",
      "amisan\n",
      "eriz\n",
      "elso\n",
      "amee\n",
      "erymoren\n",
      "etti\n",
      "ara\n",
      "hilan\n",
      "ryena\n",
      "ecelieannia\n",
      "enbiyrta\n",
      "mhsk\n",
      "on\n",
      "ilandyck\n",
      "eno\n",
      "troon\n",
      "tnen\n",
      "reehara\n",
      "aminkanuena\n",
      "htlen\n",
      "aracaely\n",
      "ca\n",
      "ehansiel\n",
      "te\n",
      "imi\n",
      "ansila\n",
      "alenee\n",
      "hdnlyigantonson\n",
      "oy\n",
      "zanyriy\n",
      "yey\n",
      "ri\n",
      "rnlinistartiiyt\n",
      "oriane\n",
      "ore\n",
      "ad\n",
      "adea\n",
      "armon\n",
      "mmytkeebaniah\n"
     ]
    }
   ],
   "source": [
    "# sample from the model\n",
    "sample_gen = torch.Generator().manual_seed(2147483647 + 10)\n",
    "\n",
    "for _ in range(100):\n",
    "    \n",
    "    out = []\n",
    "    context = [0] * (context_window - 1) # context = [. .]\n",
    "    start_char_idx = torch.multinomial(torch.tensor([1] * 26) / 26, num_samples=1, replacement=True).item() + 1\n",
    "    context.append(start_char_idx)\n",
    "    # print(''.join(itos[i] for i in context))\n",
    "    while True:\n",
    "      emb = char_vectors[torch.tensor([context])] # (1,block_size,d)\n",
    "      tan_h = torch.tanh(emb.view(1, -1) @ W1 + B1)\n",
    "      output = tan_h @ W2 + B2\n",
    "      probs = torch.nn.functional.softmax(output, dim=1)\n",
    "      ix = torch.multinomial(probs, num_samples=1, generator=sample_gen).item()\n",
    "      context = context[1:] + [ix]\n",
    "      if ix == 0:\n",
    "        break\n",
    "      \n",
    "      out.append(ix)\n",
    "    \n",
    "    print(''.join(itos[i] for i in out))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
