{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from GPT import Block\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "print_interval = 100\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainset = torchvision.datasets.CIFAR10(root='./data/cifar-10-train', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "testset = torchvision.datasets.CIFAR10(root='./data/cifar-10-test', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(testset.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Convert the image into patches and then project them into a vector space.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image_size, patch_size, num_channels, embed_dim):\n",
    "        super().__init__()\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_channels = num_channels\n",
    "        self.embed_dim = embed_dim\n",
    "        # Calculate the number of patches from the image size and patch size\n",
    "        self.num_patches = (self.image_size // self.patch_size) ** 2\n",
    "        # Create a projection layer to convert the image into patches\n",
    "        # The layer projects each patch into a vector of size hidden_size\n",
    "        self.projection = nn.Conv2d(self.num_channels, self.embed_dim, kernel_size=self.patch_size, stride=self.patch_size)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # (batch_size, num_channels, image_size, image_size) -> (batch_size, num_patches, hidden_size)\n",
    "        x = self.projection(x)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "\n",
    "    \n",
    "    def __init__(self, image_size: int, patch_size: int, num_channels: int, embed_dim: int, num_classes: int):\n",
    "        super().__init__()\n",
    "       \n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_channels = num_channels\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.num_patches = (self.image_size // self.patch_size) ** 2 # Num patches == Context\n",
    "        self.mask = torch.zeros(size = (self.num_patches + 1, self.num_patches + 1)).bool().to(device=device)\n",
    "\n",
    "        # Token embedding table is used for token identification encoding\n",
    "        # Position embedding table is used for token position (in reference to the current context) encoding\n",
    "        self.patch_embeddings = PatchEmbeddings(image_size, patch_size, num_channels, embed_dim).to(device=device)\n",
    "        self.cls_token = nn.Parameter(data = torch.randn(size=(1,1, embed_dim), device=device))\n",
    "        self.position_embedding_table = nn.Embedding(self.num_patches + 1, embed_dim, device=device)\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(emb_dims=embed_dim, num_heads=4),\n",
    "            Block(emb_dims=embed_dim, num_heads=4),\n",
    "            Block(emb_dims=embed_dim, num_heads=4),\n",
    "        ])\n",
    "\n",
    "        # Final layer norm\n",
    "        self.ln_f = nn.LayerNorm(embed_dim) \n",
    "        \n",
    "        # Language model head used for output\n",
    "        self.lm_head = nn.Linear(embed_dim, self.num_classes)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        B, C, H, W = x.shape\n",
    "\n",
    "        # x and targets are both (B,C) tensor of integers\n",
    "\n",
    "        # Getting the Patch embeddings\n",
    "        patch_emb: torch.Tensor = self.patch_embeddings(x) # (B,C,D)\n",
    "        cls_token = self.cls_token.expand(B, -1, -1)\n",
    "\n",
    "        # Added the Class token to the Patch embeddings\n",
    "        x = torch.concat([cls_token, patch_emb], dim=1) # (B, C+1, D) Added Class token\n",
    "        \n",
    "        B, C, D = x.shape\n",
    "\n",
    "        # Getting the position embedding for all the positions, starting from 0 -> context - 1\n",
    "        pos_emb = self.position_embedding_table(torch.arange(C, device=device)) # (C,D)\n",
    "\n",
    "        # Adding the position embedding to the patch embeddings \n",
    "        x = x + pos_emb\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x, self.mask)\n",
    "            \n",
    "        x = self.ln_f(x) \n",
    "        logits = self.lm_head(x)\n",
    "        cls_logits = logits[:, 0]\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            loss = functional.cross_entropy(cls_logits, targets)\n",
    "\n",
    "        return cls_logits, loss\n",
    "\n",
    "    def predict(self, x):\n",
    "        # Get the predictions\n",
    "        cls_logits, loss = self.forward(x)\n",
    "        probs = functional.softmax(cls_logits, dim=-1)\n",
    "        predictions = probs.argmax(dim = -1)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.60993 M parameters\n"
     ]
    }
   ],
   "source": [
    "model = VisionTransformer(\n",
    "    image_size=32,\n",
    "    patch_size=4,\n",
    "    num_channels=3,\n",
    "    embed_dim=128,\n",
    "    num_classes=10\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Print the number of parameters in the model\n",
    "print(sum(param.numel() for param in model.parameters()) / 1e6, 'M parameters')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = torch.load('vit.pt')\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6250, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Test function\n",
    "acc = []\n",
    "for step, batch in enumerate(testloader):\n",
    "  \n",
    "  # every once in a while evaluate the loss on train and val sets\n",
    "  if step % print_interval == 0 :\n",
    "      print(f\"step {step}: acc {np.array(acc).mean()}\")  \n",
    "  x, y = batch\n",
    "  x = x.to(device)\n",
    "  y = y.to(device)\n",
    "  preds = model.predict(x)\n",
    "  scores = torch.eq(y, preds).float()\n",
    "  acc.append(scores.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8f1ebdf36fc4f6e988cef91b3cf88ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 0.0\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "\n",
    "progress_bar = tqdm(range(epochs * len(trainloader)))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for step, batch in enumerate(trainloader):\n",
    "        # every once in a while evaluate the loss on train and val sets\n",
    "        if step % print_interval == 0 :\n",
    "            print(f\"step {step}: train loss {total_loss / (step + 1)}\")\n",
    "\n",
    "        x, y = batch\n",
    "        # evaluate the loss\n",
    "        logits, loss = model.forward(x = x, targets =y)\n",
    "        total_loss += loss.item()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        progress_bar.update(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
