{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional\n",
    "from torch.utils.data import Dataset as torchDataset, DataLoader\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer functions\n",
    "class ByteTokenizer:\n",
    "  def __init__(self, chars: 'list[str]') -> None:\n",
    "    # Map creation\n",
    "    self.stoi = {}\n",
    "    self.itos = {}\n",
    "    for idx, char in enumerate(chars):\n",
    "      self.stoi[char] = idx\n",
    "      self.itos[idx] = char\n",
    "    pass\n",
    "  \n",
    "  def encode(self, text: str):\n",
    "    output = list(range(len(text)))\n",
    "    for idx, char in enumerate(text):\n",
    "      output[idx] = self.stoi[char]\n",
    "\n",
    "    return output\n",
    "\n",
    "  def decode(self, arr: list[int]):\n",
    "    output = list(range(len(arr)))\n",
    "    for idx in range(len(arr)):\n",
    "      output[idx] = self.itos[arr[idx]]\n",
    "\n",
    "    return \"\".join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single Self Attention Head\n",
    "class SelfAttentionHead(nn.Module):\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.head_size = head_size\n",
    "        # Key, Query and Value weights are (D, H)\n",
    "        self.key = nn.Linear(emb_dims, head_size, bias=False)\n",
    "        self.query = nn.Linear(emb_dims, head_size, bias=False)\n",
    "        self.value = nn.Linear(emb_dims, head_size, bias=False)\n",
    "\n",
    "\n",
    "    # Input is (B, C, D) ; Output is (B, C, H)\n",
    "    def forward(self, key_input: torch.Tensor, query_input: torch.Tensor, value_input: torch.Tensor, mask: torch.Tensor):\n",
    "        B, C, D = key_input.shape # Batch, Context, Dimensionality\n",
    "        key: torch.Tensor =  self.key(key_input) # (B, C, D) @ (B, D, H) -> (B, C, H)\n",
    "        query: torch.Tensor = self.query(query_input) # (B, C, D) @ (B, D, H) -> (B, C, H)\n",
    "        value: torch.Tensor  = self.value(value_input) # (B, C, D) @ (B, D, H) -> (B, C, H)\n",
    "        wei: torch.Tensor =  query @ key.transpose(-2, -1) * self.head_size **-0.5 # (B, C, H) @ (B, H, C) => (B, C, C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "\n",
    "        wei = wei.masked_fill(mask, float('-inf')) # (B, C, C)\n",
    "        wei = functional.softmax(wei, dim=-1) # (B, C, C)\n",
    "\n",
    "        # perform the weighted aggregation of the values\n",
    "\n",
    "        out = wei @ value # (B, C, C) @ (B, C, H) -> (B, C, H)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple Self Attention Heads in Parallel\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "\n",
    "    def __init__(self, num_heads: int, head_size: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # Key, Query and Value weights are (D, H)\n",
    "        self.num_heads = num_heads\n",
    "        self.head_size = head_size\n",
    "        self.emb_dim = num_heads * head_size # Dimensionality \n",
    "        self.query = nn.Linear(self.emb_dim, self.emb_dim, bias=False)\n",
    "        self.value = nn.Linear(self.emb_dim, self.emb_dim, bias=False)\n",
    "        self.key =   nn.Linear(self.emb_dim, self.emb_dim, bias=False)\n",
    "        self.value_proj = nn.Linear(self.emb_dim, self.emb_dim) # Additional layer for inter head communication\n",
    "\n",
    "    def split(self, x:torch.Tensor):\n",
    "        B, C, D = x.shape\n",
    "        x = x.view(B, C, self.num_heads, self.head_size)\n",
    "        return x.permute(0, 2, 1, 3) # B, N, C, H\n",
    "\n",
    "    def forward(self, query_input: torch.Tensor, key_input: torch.Tensor, value_input: torch.Tensor,mask = None):\n",
    "\n",
    "        B, C, D = value_input.shape\n",
    "\n",
    "        if mask is None:\n",
    "            # Default mask is the encoder mask\n",
    "            mask = torch.zeros(size = (C, C)).bool().to(device=value_input.device)\n",
    "\n",
    "        # B, N, C, H in size\n",
    "        query= self.split(self.query(query_input))\n",
    "        key = self.split(self.key(key_input))\n",
    "        value = self.split(self.value(value_input))\n",
    "\n",
    "        wei = query @ key.transpose(-2, -1) * self.head_size ** -0.5 # (B, N, C, H) @ (B, N, H, C) => (B, N, C, C)\n",
    "\n",
    "        wei = wei.masked_fill(mask, float('-inf')) # (B, N, C, C)\n",
    "        wei = functional.softmax(wei, dim=-1) # (B, N, C, C)\n",
    "        values = wei @ value # (B, N, C, C) @ (B, N, C, H) -> (B, N, C, H)\n",
    "        values = values.permute(0, 2, 1, 3) # (B, C, N, H)\n",
    "        values = values.reshape(B, C, self.emb_dim)\n",
    "        values = self.value_proj(values)\n",
    "\n",
    "        return values, wei\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Simple Linear Layer with ReLU for adding computational abilities\n",
    "class FeedFoward(nn.Module):\n",
    "\n",
    "    def __init__(self, emb_dims):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential( \n",
    "            nn.Linear(emb_dims, 4 * emb_dims), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * emb_dims, emb_dims),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Block: Communication followed by Computation \n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, emb_dims, num_heads):\n",
    "        # emb_dims: embedding dimension, num_heads: the number of heads we'd like\n",
    "        super().__init__()\n",
    "\n",
    "        # Divide the embedding dimensions by the number of heads to get the head size\n",
    "        head_size = emb_dims // num_heads\n",
    "\n",
    "        # Communication\n",
    "        self.self_att = MultiHeadAttention(num_heads, head_size)\n",
    "\n",
    "        # Computation\n",
    "        self.feed_fwd = FeedFoward(emb_dims)\n",
    "\n",
    "        # Adding Layer Normalization\n",
    "        self.ln1 = nn.LayerNorm(emb_dims)\n",
    "        self.ln2 = nn.LayerNorm(emb_dims)\n",
    "\n",
    "\n",
    "    def forward(self, x, mask: torch.Tensor):\n",
    "        # Residual connections allow the network to learn the simplest possible function. No matter how many complex layer we start by learning a linear function and the complex layers add in non linearity as needed to learn true function.\n",
    "        val, attention = self.self_att.forward(self.ln1(x), self.ln1(x), self.ln1(x), mask)\n",
    "        x = x + val\n",
    "        x = x + self.feed_fwd.forward(self.ln2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, context, emb_dims, vocab_size, form = \"decoder\"):\n",
    "        super().__init__()\n",
    "       \n",
    "       \n",
    "        if form == 'decoder':\n",
    "            self.mask = torch.triu(torch.full(size = (context,context), fill_value= -torch.inf), diagonal=1).bool().to(device=device)\n",
    "        else:\n",
    "            self.mask = torch.zeros(size = (context, context)).bool().to(device=device)\n",
    "        self.context = context\n",
    "        self.emb_dims = emb_dims\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        # Token embedding table is used for token identification encoding\n",
    "        # Position embedding table is used for token position (in reference to the current context) encoding\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, emb_dims)\n",
    "        self.position_embedding_table = nn.Embedding(context, emb_dims)\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(emb_dims=emb_dims, num_heads=4),\n",
    "            Block(emb_dims=emb_dims, num_heads=4),\n",
    "            Block(emb_dims=emb_dims, num_heads=4),\n",
    "        ])\n",
    "\n",
    "        # Final layer norm\n",
    "        self.ln_f = nn.LayerNorm(emb_dims) \n",
    "        \n",
    "        # Language model head used for output\n",
    "        self.lm_head = nn.Linear(emb_dims, vocab_size)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        B, C = x.shape\n",
    "\n",
    "        # x and targets are both (B,C) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(x) # (B,C,D)\n",
    "\n",
    "        # Getting the position embedding for all the positions, starting from 0 -> context - 1\n",
    "        pos_emb = self.position_embedding_table(torch.arange(C, device=\"cuda\")) # (C,D)\n",
    "        x = tok_emb + pos_emb\n",
    "        for block in self.blocks:\n",
    "            x = block(x, self.mask)\n",
    "        x = self.ln_f(x) \n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, C, D = logits.shape\n",
    "            logits = logits.view(B*C, D)\n",
    "            targets = targets.view(B*C)\n",
    "            loss = functional.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, C) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "\n",
    "            # crop idx to the last context \n",
    "            idx_cond = idx[:, -self.context:]\n",
    "\n",
    "            # Get the predictions\n",
    "            logits, loss = self.forward(x=idx_cond)\n",
    "\n",
    "            # Focus only on the last step which contains the output considering the entire context window\n",
    "            # logits are (batch_size, context = full context considered time step, dimensionality) which is essentially the output vector for each batch\n",
    "            logits = logits[:, -1, :] \n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = functional.softmax(logits, dim=1)\n",
    "\n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "\n",
    "            # Appended along the context_window hence the context keeps building up\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (batch_size, context_window + 1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True NVIDIA GeForce RTX 3060\n"
     ]
    }
   ],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "print(cuda, torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "context = 256\n",
    "emb_dims = 128\n",
    "print_interval = 500\n",
    "device = 'cuda' if cuda else 'cpu'\n",
    "max_iters = 5000\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the file\n",
    "file = open('input.txt', 'r', encoding='utf-8')\n",
    "text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ByteTokenizer(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torchDataset):\n",
    "    def __init__(self, text: str) -> None:\n",
    "      self.data = tokenizer.encode(text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "      x = self.data[index : index + context]\n",
    "      y = self.data[index + 1 : index + context + 1]\n",
    "\n",
    "      return torch.tensor(x).to(device), torch.tensor(y).to(device)\n",
    "    def __len__(self):\n",
    "      return len(self.data) - context - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(text=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.643393 M parameters\n"
     ]
    }
   ],
   "source": [
    "model = Transformer(context=context, emb_dims=emb_dims, vocab_size=vocab_size).to(device=device)\n",
    "\n",
    "# Print the number of parameters in the model\n",
    "print(sum(param.numel() for param in model.parameters()) / 1e6, 'M parameters')\n",
    "\n",
    "# Create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cb59d05e03b495b8304296006bf2c13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34849 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 0.0\n",
      "step 500: train loss 2.4424705053279974\n",
      "step 1000: train loss 2.1932401951257283\n",
      "step 1500: train loss 2.0210089710535484\n",
      "step 2000: train loss 1.9033647016666342\n",
      "step 2500: train loss 1.8184292354568488\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "\n",
    "total_loss = 0\n",
    "progress_bar = tqdm(range(len(dataloader)))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        # every once in a while evaluate the loss on train and val sets\n",
    "        if step % print_interval == 0 :\n",
    "            print(f\"step {step}: train loss {total_loss / (step + 1)}\")\n",
    "\n",
    "        x, y = batch\n",
    "        # evaluate the loss\n",
    "        logits, loss = model.forward(x = x, targets =y)\n",
    "        total_loss += loss.item()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "start = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "open('more.txt', 'w').write(tokenizer.decode(model.generate(start, max_new_tokens=10000)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
